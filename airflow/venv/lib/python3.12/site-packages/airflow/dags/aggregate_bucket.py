from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

dag = DAG(
    dag_id='aggregate_bucket',
    start_date=datetime(2024, 4, 4),
    schedule='@daily',
    catchup=False
    )

process_raw_data = BashOperator(
    task_id='process_raw_data',
    bash_command='source /dis/test_spark/venv/bin/activate & python /dis/data_storage/consumer/tables/curated_bucket/write_to_curated_bucket.py',
    dag=dag
)

generate_gold_data = BashOperator(
    task_id = 'generate_gold_datadata',
    bash_command='source /dis/test_spark/venv/bin/activate & python /dis/data_storage/consumer/tables/gold_bucket/write_to_gold_bucket.py',
    dag=dag
)